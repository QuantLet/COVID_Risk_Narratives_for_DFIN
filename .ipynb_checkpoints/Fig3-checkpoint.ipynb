{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This program is used to generate word embeddings on a rolling basis\n",
    "the results are used to generate Fig3.\n",
    "Autho: Yuting Chen\n",
    "\"\"\"\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path =\"/Input/\"\n",
    "df = pd.read_pickle(path+'Mbf_corona_updated_24042021.pkl')\n",
    "df = df.set_index('date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"clean and tokenize data\"\"\"\n",
    "import preprocessor.api as p\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def pre_process(x):\n",
    "    # remove website links\n",
    "    y = re.sub(r\"http\\S+\", \"\", str(x))\n",
    "    # remove multiple spaces\n",
    "    y = re.sub(r'\\s+', ' ', y)\n",
    "    # remove multiple lines\n",
    "    y = re.compile(r'\\n+').sub(' ', y)    \n",
    "    # Removing html tags\n",
    "    y = re.compile(r'<[^>]+>').sub('', y)\n",
    "    # Remove punctuations and numbers\n",
    "    y = re.sub('[^a-zA-Z]', ' ', y)\n",
    "    # Single character removal\n",
    "    y = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', y)\n",
    "    # Removing multiple spaces\n",
    "    y = re.sub(r'\\s+', ' ', y)    \n",
    "    # cleaning using processor for tweets\n",
    "    y = p.clean(y)\n",
    "    y = y.lower()\n",
    "    # tokenize and remove stopwords\n",
    "    word_tokens = word_tokenize(y) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return filtered_sentence \n",
    "\n",
    "if data_type ==1:\n",
    "    list_text = df.CONTENT.tolist()\n",
    "else:\n",
    "    list_text = df.text.tolist()\n",
    "data = [] \n",
    "for text in list_text:\n",
    "    temp = pre_process(text)\n",
    "    data.append(temp)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['CONTENT_P'] = data\n",
    "df = df[['CONTENT_P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# I run the embedding on a rolling basis, 5h 42min 56s for 50 day rolling\n",
    "trained_models = {}\n",
    "\n",
    "# # select time range\n",
    "min_date = df.index.min()\n",
    "max_date = df.index.max()\n",
    "\n",
    "# for testing\n",
    "# max_date = pd.to_datetime('2020-3-01')\n",
    "start_date = min_date\n",
    "end_date = start_date + pd.DateOffset(days=50)\n",
    "\n",
    "while end_date <= max_date:\n",
    "    print(start_date,end_date)\n",
    "    start_date = start_date + pd.DateOffset(days=1)\n",
    "    end_date = start_date + pd.DateOffset(days=50)\n",
    "    df_s = df[start_date:end_date]\n",
    "    model = gensim.models.Word2Vec(df_s.CONTENT_P, vector_size=100, min_count = 5, window = 5, workers = 8) \n",
    "    trained_models[end_date]=model\n",
    "    print(\"Cosine similarity between 'coronavirus' \" + \n",
    "               \"and 'crisis' - CBOW : \", \n",
    "    model.wv.similarity('coronavirus', 'crisis')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"jump to here directly if I have trained and saved the models previously\"\"\"\n",
    "# save the models (if first time run it) or load them\n",
    "# files too large 12.5 g in total, I need to save them on disk\n",
    "from datetime import datetime\n",
    "# this part is used to save the trained models to a folder\n",
    "# for date,model in trained_models.items():\n",
    "#     name = \"Temp/Embedding/\"+date.strftime(\"%Y%m%d\")+\"embedding_covidnews.model\"\n",
    "#     model.save(name)\n",
    "    \n",
    "# select time range\n",
    "min_date = df.index.min()\n",
    "max_date = df.index.max()\n",
    "start_date = min_date\n",
    "end_date = start_date + pd.DateOffset(days=50)\n",
    "l_dates = []\n",
    "while end_date <= max_date:\n",
    "    start_date = start_date + pd.DateOffset(days=1)\n",
    "    end_date = start_date + pd.DateOffset(days=50)\n",
    "    l_dates.append(end_date)\n",
    "    \n",
    "# this part is used to load the trained models and save them to a list\n",
    "for date in l_dates:\n",
    "    name = select_path()+\"Temp/Embedding/\"+date.strftime(\"%Y%m%d\")+\"embedding_covidnews.model\"\n",
    "    model = Word2Vec.load(name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results \n",
    "df_sim = pd.DataFrame()\n",
    "list_date = []\n",
    "list_sim1 = []\n",
    "list_sim2 = []\n",
    "\n",
    "for date,model in trained_models.items():\n",
    "    list_date.append(date)\n",
    "    list_sim1.append(model.wv.similarity('covid', 'crisis'))\n",
    "    list_sim2.append(model.wv.similarity('covid', 'recession'))\n",
    "\n",
    "# print(\"most similar words with coronavirus : \", \n",
    "#       model1.wv.most_similar('coronavirus')[:10])\n",
    "df_sim['date'] = list_date\n",
    "df_sim['covid_crisis'] = list_sim1\n",
    "df_sim['covid_recession'] = list_sim2\n",
    "\n",
    "df_sim = df_sim.set_index('date')\n",
    "df_sim.plot()  \n",
    "print(df_sim.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word, size):\n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    close_words = model.wv.similar_by_word(word,topn=50)\n",
    "#     print(close_words)\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    fig = plt.figure(1, figsize=(10, 6))\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points',size=10)\n",
    "        plt.xlim(x_coords.min()+0.05, x_coords.max()+0.05)\n",
    "        plt.ylim(y_coords.min()+0.05, y_coords.max()+0.05)\n",
    "    plt.show()\n",
    "#     fig.savefig('tsne_tweets.png',bbox_inches=None)\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib. pyplot as plt\n",
    "display_closestwords_tsnescatterplot(model1, 'coronavirus', 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine similarity between 'coronavirus' \" + \n",
    "               \"and 'worse' - CBOW : \", \n",
    "    model1.similarity('trump', 'worse')) \n",
    "print(\"Cosine similarity between 'coronavirus' \" + \n",
    "               \"and 'good' - CBOW : \", \n",
    "    model1.similarity('trump', 'good')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.similar_by_word('coronavirus',topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
